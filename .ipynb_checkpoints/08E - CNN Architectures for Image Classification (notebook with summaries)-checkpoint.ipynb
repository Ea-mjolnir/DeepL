{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42a5dd0-6b31-41aa-9349-5e2ce073070f",
   "metadata": {},
   "source": [
    "# Exercise 08 - CNN Architectures for Image Classification\n",
    "\n",
    "In this notebook, it is shown how to implement complex CNN architectures (for image classification) in a simple and flexible way. For this, we use the subclassing API to group together a sequence/collection of layers into more complex layers (or modules), so that we are able to reuse them many times. At the same time, we need the functional API to describe the flow of data (tensors) through the layers.\n",
    "\n",
    "**Learning objectives:**\n",
    "- Learn how to build more complex neural network architectures\n",
    "- Get to know the Subclassing and Functional API of TensorFlow\n",
    "- Practice to implement some modules and architecture by yourself\n",
    "\n",
    "**The constructed neural network architectures are just constructed in this notebook, but not trained in any way. Check and compare your solutions with the network summaries in the accompanying PDF notebook version.**\n",
    "\n",
    "Please note that we have implemented the suggested solution carefully and to the best of our knowledge. If your solution looks slightly different, then your solution is not necessarily wrong. Talk to us and we will check what the differences could be. We, too, make mistakes.\n",
    "\n",
    "**Before you continue, find a GPU on the system that is not heavily used by other users (with nvidia-smi), and change X to the id of this GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7916221-310d-48c9-a428-e7cfb14efa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change X to the GPU number you want to use,\n",
    "# otherwise you will get a Python error\n",
    "# e.g. USE_GPU = 4\n",
    "USE_GPU = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6cb2d6-cb81-43d1-87f7-d8e02a005f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 07:41:58.405406: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 07:41:59.255943: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "\n",
      "Available GPU Devices:\n",
      "  /physical_device:GPU:0 GPU\n",
      "  /physical_device:GPU:1 GPU\n",
      "  /physical_device:GPU:2 GPU\n",
      "  /physical_device:GPU:3 GPU\n",
      "  /physical_device:GPU:4 GPU\n",
      "  /physical_device:GPU:5 GPU\n",
      "  /physical_device:GPU:6 GPU\n",
      "  /physical_device:GPU:7 GPU\n",
      "\n",
      "Visible GPU Devices:\n",
      "  /physical_device:GPU:4 GPU\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow \n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the installed TensorFlow version\n",
    "print(f'TensorFlow version: {tf.__version__}\\n')\n",
    "\n",
    "# Get all GPU devices on this server\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all GPU devices\n",
    "print('Available GPU Devices:')\n",
    "for gpu in gpu_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set only the GPU specified as USE_GPU to be visible\n",
    "tf.config.set_visible_devices(gpu_devices[USE_GPU], 'GPU')\n",
    "\n",
    "# Get all visible GPU  devices on this server\n",
    "visible_devices = tf.config.get_visible_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all visible GPU devices\n",
    "print('\\nVisible GPU Devices:')\n",
    "for gpu in visible_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set the visible device(s) to not allocate all available memory at once,\n",
    "# but rather let the memory grow whenever needed\n",
    "for gpu in visible_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdaa047-59a8-4483-9530-e1ba300e9ae3",
   "metadata": {},
   "source": [
    "## ResNet34\n",
    "\n",
    "The following example shows how to implement the ResNet CNN architecture with 34 (trainable) layers. The ResNet architecture uses residual blocks that are repeated several times that only differ in the number of convolutional filters these blocks use, and the use of a stride of 2 when the number of filters change in order to shrink the activation volumes. We therefore define a new module (as a TensorFlow/Keras layer) that we can use over and over again, and that are parameterized in this exact way. Besides the residual blocks, the ResNet architecture has a stump (at the beginning) that quickly shrinks the image with a large filter size, and a classifier head (at the end).\n",
    "\n",
    "The implementation therefore follows these three steps:\n",
    "1. Define a residual block module that can be configured and re-used in the main block with the Subclassing API\n",
    "2. Construct a neural network stump model using the Sequential API (although we could also use the Functional API for this)\n",
    "3. Connect the stump with a sequence of configured residual blocks\n",
    "4. Add a classifier as the head\n",
    "\n",
    "Please note that the following implementation is very simplified and only exposes the necessary configurations (filter number, stride) and abstractions for the architectures of this notebook. Typically, one would probably also allow some more flexibility like the configuration of the activation function or the kernel initializer. And often, layers that are repeated several times in a module, like the Conv2D layer, would be specified in a new object, so that the specifications of that particular layer can be changed at only one place. For example, the Conv2D layer is repeated with the same configuration in the residual block three times. If we would like to change something, e.g. the activation function of the Conv2D layer to use leaky ReLU instead of ReLU, then we would need to change it also at three different locations, which could lead to errors, because we must not forget any of these three layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb66bb-2fa0-4300-ba15-b28ec0e69e3e",
   "metadata": {},
   "source": [
    "**Define the residual block layer**\n",
    "\n",
    "All neural networks of the ResNet family consist in their center part of several stacked residual blocks, which differ only in their number of filters. Therefore, using the subclassing API of TensorFLow, we define a new Keras layer class called `ResidualBlock` that is a subclass of the superclass `Layer` (of the layers module of Keras). Then, we can use this residual block over and over again. \n",
    "\n",
    "For such layer classes, we need to define the constructor (`__init()__`) that is called when the object of that class is constructed, and the method `call()` that is called in the forward pass (of the network that contains this residual block).\n",
    "\n",
    "In the `__init()__` method, the layers of the main path, and of the skip connection are defined:\n",
    "- The main path is pretty straight-forward and is just a sequence of a 2D convolutional layer, batch normalization, activation function (typically ReLU), another 2D convolutional layer, and batch normalization.\n",
    "- The skip connection path is typically empty and the input of this path is equal to its output. Only when the stride of the residual block is greater than one, then the size of the activation volume shrinks in the main path, and the skip connection must also shrink the activation volume accordingly, as otherwise they cannot be added together at the end of this block.\n",
    "As you might notice, the layers are at this point just constructed and stored (as a sequence) in Python lists. (With the exception of the last ReLU activation function, which is just stored in a variable.) The layers are at this point not connected in any way.\n",
    "\n",
    "In the `call()` method, the path of the inputs through the layers is defined. For one, the input goes through the layers of the main path, and for another, through the skip connection. (If the skip connection path is empty, the input remains unchanged, otherwise it shrinks the activation volume.) The resulting tensors from the two paths are then added together, and the result goes one more time through an activation function. In this method, we basically call the layers (contained in the list we constructed in the constructor method) in the right order using the input of the neural network that is provided to this method. The output of calling a layer object is then the input of the next layer. So, we loop through the layers in the lists, call the layer with the input (which might be the output of the previous layer), and store again the output of this layer in the variable, which is then used in the next iteration as the input of the next layer. Before the last ReLU activation function is called, the results from the main path and from the skip connection path are (elemenwise) added with the `+` operator. This is how we define the data (tensor) flow through the layers (and therefore the neural network architecture) with the Functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab82526-2211-4ee6-ace3-0d3f3dd5ab4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU\n",
    "\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, filters, strides=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "                \n",
    "        # layers of main path\n",
    "        self.main_layers = [\n",
    "            Conv2D(filters, kernel_size=3, strides=strides, padding='same', kernel_initializer='he_normal', use_bias=False),            \n",
    "            BatchNormalization(),\n",
    "            ReLU(),\n",
    "            Conv2D(filters, kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal', use_bias=False),            \n",
    "            BatchNormalization()\n",
    "        ]\n",
    "        \n",
    "        # layers of skip connection path\n",
    "        self.skip_layers = []\n",
    "        \n",
    "        # if the stride is greater than 1, then use a kernel size\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                Conv2D(filters, kernel_size=1, strides=strides, padding='same', kernel_initializer='he_normal', use_bias=False),            \n",
    "                BatchNormalization()\n",
    "            ]\n",
    "                        \n",
    "        self.activation = ReLU()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # main path\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "            \n",
    "        # skip connection path\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "            \n",
    "        # add the two results,\n",
    "        # and apply activation function\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bae74e-9489-45dd-a60c-6c1a237e2a7c",
   "metadata": {},
   "source": [
    "**Construct the network stump**\n",
    "\n",
    "Next, we define the ResNet stump that takes the input, applies batch normalization, the ReLU activation function, and max pooling. For this, we use for simplicity the Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0c4e55-2504-4042-8e30-a2ba318ae0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 07:42:05.833122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14744 MB memory:  -> device: 4, name: Quadro RTX 5000, pci bus id: 0000:81:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import MaxPool2D\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Conv2D(64, kernel_size=7, strides=2, padding='same', kernel_initializer='he_normal', use_bias=False, input_shape=[224, 224, 3]),   \n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    MaxPool2D(pool_size=3, strides=2, padding='same')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f7f73-8aa2-425b-9c0f-daafae6c016f",
   "metadata": {},
   "source": [
    "**Construct and connect the residual blocks**\n",
    "\n",
    "At this point, we want to construct and connect a number of residual blocks to the network stump. In the ResNet34 network, there are **3 blocks of 64 filters, then 4 blocks of 128 filters, 6 blocks of 256 filters, and 3 blocks of 512 filters**. We therefore define a list where there is one entry for each residual block that gives the number of filters. For this purpose, we can use the `*` operator that is defined for lists to repeat the number of elements in the list according to the integer value. For the filter numbers of the residual block, we define the list as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cee4f047-e836-4b54-90d9-da16c69ec7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 64, 64, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 512, 512, 512]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[64] * 3 + [128] * 4 + [256] * 6 + [512] * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17106bea-544a-43d4-98b4-a41c88eb3f22",
   "metadata": {},
   "source": [
    "We now just need to iterate through this list, construct a residual block with the number of filters from the current list element, and add the residual block to the model defined above.\n",
    "\n",
    "There is just one more thing: Whenever the number of filters increase, ResNet decreases the size of the activation volume by using a stride of 2 instead of 1. To accomodate for this, we keep the previous filter size, compare it with the current filter size, and if they are not the same, we use a stride of 2, otherwise a stride of 1. (We use the short if-then-else syntax of Python to have a one-liner of code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c35322a9-9ecf-4276-820a-7ce4593ec3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prev_filters = 64\n",
    "\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualBlock(filters, strides=strides))\n",
    "    prev_filters = filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61251de7-bd52-46f6-b1d4-dcb401df1ede",
   "metadata": {},
   "source": [
    "Alternatively, we could keep track of the residual block number in the for-loop using `enumerate()`, and give the block a meaningful name.\n",
    "\n",
    "(Make sure you do not call cells that add layers to models several times, as you would add layers over and over again. Not only will your model unintentionally get bigger and bigger, but you may also get errors if the layers don't match each other in terms of their configuration. If you need to change something or want to use the following alternative cell, then construct the sequential model once more.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ef71f9-f917-4dd9-b6ba-d9f029b8c04b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Conv2D(64, kernel_size=7, strides=2, padding='same', kernel_initializer='he_normal', use_bias=False, input_shape=[224, 224, 3]),   \n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    MaxPool2D(pool_size=3, strides=2, padding='same')\n",
    "])\n",
    "\n",
    "prev_filters = 64\n",
    "\n",
    "for i, filters in enumerate([64] * 3 + [128] * 4 + [256] * 6 + [512] * 3):\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualBlock(filters, strides=strides, name=f'ResBlock_{i+1:02}'))\n",
    "    prev_filters = filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93674e0f-b437-4bf5-9648-9679effe7d30",
   "metadata": {},
   "source": [
    "**Add classifier head**\n",
    "\n",
    "What is missing is the classifier head, where global average pooling is applied on the activation volume resulting from the last residual block, then flattens the result, and uses a dense layer to produce the, e.g., ten class scores probabilities (using the softmax function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be0fa26-a62c-4a58-aa70-b8f2f2c685ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAvgPool2D, Flatten, Dense\n",
    "\n",
    "model.add(GlobalAvgPool2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4bbf4a-db27-4698-9d60-b1ba720e1fb2",
   "metadata": {},
   "source": [
    "Check with the `summary()` method that the architecture of the ResNet34 model is really like you expect it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2ea249-d15e-4a36-98f2-61b95cd3f5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_36 (Conv2D)          (None, 112, 112, 64)      9408      \n",
      "                                                                 \n",
      " batch_normalization_36 (Bat  (None, 112, 112, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_33 (ReLU)             (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 56, 56, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " ResBlock_01 (ResidualBlock)  (None, 56, 56, 64)       74240     \n",
      "                                                                 \n",
      " ResBlock_02 (ResidualBlock)  (None, 56, 56, 64)       74240     \n",
      "                                                                 \n",
      " ResBlock_03 (ResidualBlock)  (None, 56, 56, 64)       74240     \n",
      "                                                                 \n",
      " ResBlock_04 (ResidualBlock)  (None, 28, 28, 128)      230912    \n",
      "                                                                 \n",
      " ResBlock_05 (ResidualBlock)  (None, 28, 28, 128)      295936    \n",
      "                                                                 \n",
      " ResBlock_06 (ResidualBlock)  (None, 28, 28, 128)      295936    \n",
      "                                                                 \n",
      " ResBlock_07 (ResidualBlock)  (None, 28, 28, 128)      295936    \n",
      "                                                                 \n",
      " ResBlock_08 (ResidualBlock)  (None, 14, 14, 256)      920576    \n",
      "                                                                 \n",
      " ResBlock_09 (ResidualBlock)  (None, 14, 14, 256)      1181696   \n",
      "                                                                 \n",
      " ResBlock_10 (ResidualBlock)  (None, 14, 14, 256)      1181696   \n",
      "                                                                 \n",
      " ResBlock_11 (ResidualBlock)  (None, 14, 14, 256)      1181696   \n",
      "                                                                 \n",
      " ResBlock_12 (ResidualBlock)  (None, 14, 14, 256)      1181696   \n",
      "                                                                 \n",
      " ResBlock_13 (ResidualBlock)  (None, 14, 14, 256)      1181696   \n",
      "                                                                 \n",
      " ResBlock_14 (ResidualBlock)  (None, 7, 7, 512)        3676160   \n",
      "                                                                 \n",
      " ResBlock_15 (ResidualBlock)  (None, 7, 7, 512)        4722688   \n",
      "                                                                 \n",
      " ResBlock_16 (ResidualBlock)  (None, 7, 7, 512)        4722688   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,306,826\n",
      "Trainable params: 21,289,802\n",
      "Non-trainable params: 17,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce837f-9060-47fe-86bd-787641e2591c",
   "metadata": {},
   "source": [
    "**Question 1: Does the constructed ResNet34 model really contain 34 layers as the name suggests?** \n",
    "\n",
    "Starting with the ResNet50 architecture (ResNet50, ResNet101, and ResNet152), the residual blocks also contain bottleneck (1x1 Conv2D) layers in the main path. The first (1x1) bottleneck layer is used **instead** of the first (3x3 Conv2D) 'regular' layer. And the second bottleneck layer comes after the second Cond2D layer, also in the main task. So, the ResNet34 architecture can be easily transformed into the ResNet50 architecture by just changing the residual block accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1eea9-29b4-4877-b492-4bf6fb343d90",
   "metadata": {},
   "source": [
    "# VGG16\n",
    "\n",
    "**Task: Implement the VGG16 neural network architecture with 'VGG blocks' in the same way as in the above example.**\n",
    "\n",
    "The VGG16 network is much simpler than the ResNet architecture, but you should also start by implementing a 'VGG block' class (called `VGGBlock`). A VGG block consists of 2 or 3 convolutional (´Conv2D´) layers, and then a max pooling (´MaxPool2D´) layer. (The VGG19 network also includes blocks of 4 convolutional layers, and this VGG block class should also be able to construct such blocks.)\n",
    "\n",
    "The constructor `__init__()` should take two parameters: one for the number of convolutional layers (where you specify if there should be 2, 3, or 4 convolutional layers), and the number of filters. Within the constructor, just construct as many `Conv2D` layers as specified in the parameter (a simple for-loop). The kernel size of the convolutional layers is (3,3), use zero padding ('same'), and ReLU ('relu') as the activation function. Since VGG does not have batch normalization layers between the convolutional layers and the activation function, you can just specify the Conv2D layers to use 'relu' as activation. The `MaxPool2D` layers use a pool size (`pool_size`) of (2,2), and strides of (2,2).\n",
    "\n",
    "In the forward pass, the `call()` method, the input is just passed through all the constructed layers, and the output of the last layer is returned. There are no parallel paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fff4228-5f40-4724-a0e7-6f8be5f49d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, ReLU, MaxPooling2D\n",
    "\n",
    "class VGGBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_conv_layers, filters, **kwargs):  \n",
    "        super(VGGBlock, self).__init__(**kwargs)\n",
    "        \n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.filters = filters\n",
    "        \n",
    "        self.conv_layers = []\n",
    "        for _ in range(1,num_conv_layers+1):\n",
    "            self.conv_layers.append(Conv2D(filters, (3, 3), use_bias=True, padding='same', activation='relu'))\n",
    "        \n",
    "        self.max_pooling = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "        x = self.max_pooling(x)\n",
    "        return x\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05283b6-ca4b-4d10-bbed-39daff3fd1c7",
   "metadata": {},
   "source": [
    "The VGG16 network does not have a real stump, and basically starts directly with a VGG block. So, it is sufficient to construct a Sequential model with only an `Input` layer that defines the input shape (`shape` parameter). (But that also depends a little on how you defined your VGG block.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "576f93c3-10ce-45f2-9edd-3779545f616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.Sequential([\n",
    "#    Conv2D(64, kernel_size=(3,3), padding='same', use_bias=True, activation=\"relu\", input_shape=[224, 224, 3]) \n",
    "#])\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "input_shape = (224, 224, 3) \n",
    "model = Sequential()\n",
    "model.add(Input(shape=input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2272c-40d2-4a1a-96f7-8a8984a18f58",
   "metadata": {
    "tags": []
   },
   "source": [
    "After the stump, there are five VGG blocks:\n",
    "- Block with 2 convolutional layers using 64 filters\n",
    "- Block with 2 convolutional layers using 128 filters\n",
    "- Block with 3 convolutional layers using 256 filters\n",
    "- Block with 3 convolutional layers using 512 filters\n",
    "- Block with 3 convolutional layers using 512 filters\n",
    "\n",
    "(Please note that the VGG16 network is often depicted with only two convolutional layers in the 3rd block. But together with the three dense layers of the classifier head, this would only result in 15 trainable layers. Same goes for the VGG19 network, where the 3rd block is often depicted wrongly.)\n",
    "\n",
    "Add these VGG blocks to the model.\n",
    "\n",
    "Note that blocks 4 and 5 cannot be merged into a block of 6 convolutional layers using 512 filters, since there is a max pooling layer after block 4 that we would otherwise miss.\n",
    "\n",
    "Since there are no real repetitions of layers, you do not need to construct a list that stored the number of layers and filter sizes first, but you can directly add the VGG blocks to the model. (If you would create a list first, you would also need to store tuples that contain these two parameter values instead of just the number of filters as above.) \n",
    "\n",
    "You could also just add the VGG blocks directly after the Input layer when you construct the Sequential model above. Same goes for the following classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed04ad37-eb64-4b3d-8fd1-30f0b3102e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.Sequential([\n",
    "#   Conv2D(64, kernel_size=(3,3), padding='same', use_bias=True, activation=\"relu\", input_shape=[224, 224, 3]), \n",
    "    \n",
    "#])\n",
    "\n",
    "model.add(VGGBlock(num_conv_layers=2, filters=64, name='vgg_block_1')),\n",
    "model.add(VGGBlock(num_conv_layers=2, filters=128, name='vgg_block_2')),\n",
    "model.add(VGGBlock(num_conv_layers=3, filters=256, name='vgg_block_3')),\n",
    "model.add(VGGBlock(num_conv_layers=3, filters=512, name='vgg_block_4')),\n",
    "model.add(VGGBlock(num_conv_layers=3, filters=512, name='vgg_block_5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af9855b-42a6-4812-9226-b0775fc3dc4a",
   "metadata": {},
   "source": [
    "And finally, the VGG16 network has a classifier head that flattens the activation volume, has two dense layers of 4096 units (using ReLU as ativation function), and one dense layer with the number of neurons as classes (here 10) and a softmax activation. Add the layers accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed922dd6-3c81-4206-99a3-184a199baa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9005a4-c922-400d-9d06-24211d1e1e02",
   "metadata": {},
   "source": [
    "Use summary to inspect and verify your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6041e620-5892-445f-ad7e-49ade8aae61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg_block_1 (VGGBlock)      (None, 112, 112, 64)      38720     \n",
      "                                                                 \n",
      " vgg_block_2 (VGGBlock)      (None, 56, 56, 128)       221440    \n",
      "                                                                 \n",
      " vgg_block_3 (VGGBlock)      (None, 28, 28, 256)       1475328   \n",
      "                                                                 \n",
      " vgg_block_4 (VGGBlock)      (None, 14, 14, 512)       5899776   \n",
      "                                                                 \n",
      " vgg_block_5 (VGGBlock)      (None, 7, 7, 512)         7079424   \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,301,514\n",
      "Trainable params: 134,301,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a108ad66-5926-418b-b984-069805fbdd55",
   "metadata": {},
   "source": [
    "When the parameter `expand_nested` is set to True, the summary also contains the nested layers (when a layer object consists of other layers). This does not alway seem to work, e.g. if there are parallel paths as in the ResNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f59d4e0e-cdff-4b26-8c8d-b0548c090f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg_block_1 (VGGBlock)      (None, 112, 112, 64)      38720     \n",
      "                                                                 \n",
      " vgg_block_2 (VGGBlock)      (None, 56, 56, 128)       221440    \n",
      "                                                                 \n",
      " vgg_block_3 (VGGBlock)      (None, 28, 28, 256)       1475328   \n",
      "                                                                 \n",
      " vgg_block_4 (VGGBlock)      (None, 14, 14, 512)       5899776   \n",
      "                                                                 \n",
      " vgg_block_5 (VGGBlock)      (None, 7, 7, 512)         7079424   \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,301,514\n",
      "Trainable params: 134,301,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f06a05-779b-4e96-8075-d2a693ae4ccf",
   "metadata": {},
   "source": [
    "**Question 2: What do you need to change to get from the VGG16 to the VGG 19 architecture?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "403f4fa7-b0bb-4e69-90cf-73d8f5bffe6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg_block_1 (VGGBlock)      (None, 112, 112, 64)      75648     \n",
      "                                                                 \n",
      " vgg_block_2 (VGGBlock)      (None, 56, 56, 128)       369024    \n",
      "                                                                 \n",
      " vgg_block_3 (VGGBlock)      (None, 28, 28, 256)       2065408   \n",
      "                                                                 \n",
      " vgg_block_4 (VGGBlock)      (None, 14, 14, 512)       8259584   \n",
      "                                                                 \n",
      " vgg_block_5 (VGGBlock)      (None, 7, 7, 512)         9439232   \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139,795,722\n",
      "Trainable params: 139,795,722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, ReLU, MaxPooling2D, Input,Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "class VGGBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_conv_layers, filters, **kwargs):  \n",
    "        super(VGGBlock, self).__init__(**kwargs)\n",
    "        \n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.filters = filters\n",
    "        \n",
    "        self.conv_layers = []\n",
    "        for _ in range(1,num_conv_layers+1):\n",
    "            self.conv_layers.append(Conv2D(filters, (3, 3), use_bias=True, padding='same', activation='relu'))\n",
    "        \n",
    "        self.max_pooling = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "        x = self.max_pooling(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "input_shape = (224, 224, 3) \n",
    "model = Sequential()\n",
    "model.add(Input(shape=input_shape))\n",
    "model.add(VGGBlock(num_conv_layers=3, filters=64, name='vgg_block_1')),\n",
    "model.add(VGGBlock(num_conv_layers=3, filters=128, name='vgg_block_2')),\n",
    "model.add(VGGBlock(num_conv_layers=4, filters=256, name='vgg_block_3')),\n",
    "model.add(VGGBlock(num_conv_layers=4, filters=512, name='vgg_block_4')),\n",
    "model.add(VGGBlock(num_conv_layers=4, filters=512, name='vgg_block_5'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039134cc-0c25-4adb-98c6-bf94c0d18bde",
   "metadata": {},
   "source": [
    "# GoogLeNet\n",
    "\n",
    "**Task: Implement the Inception module, and the GoogLeNet neural network architecture.**\n",
    "\n",
    "When implementing the Inception block, there is no real benefit of organizing the layers in lists, since the four parallel paths have either one or two layers, only. Just store them in class variables (that might be named according to the path).\n",
    "\n",
    "The Inception block does not have much variation, only in the number of filters that are used. However, there are six convolutional layers for which the number of filters are to be specified. These can be given to the Inception block class by a Python list, e.g. by `filters=[64, 96, 128, 12, 32, 32]`. Just be careful in which order you fill this list, and that you use the correct index for the respective convolutional layer. **Take a look at the definition of the main body of the architecture below.**\n",
    "\n",
    "The four paths are specified as follows:\n",
    "- Path 1: 1 convolutional layer with kernel size 1x1\n",
    "- Path 2: 1 convolutional layer with kernel size 1x1, then 1 convolutional layer with kernel size 3x3\n",
    "- Path 3: 1 convolutional layer with kernel size 1x1, then 1 convolutional layer with kernel size 5x5\n",
    "- Path 4: 1 max pooling layer with pooling size 3x3 (strides of 1), then 1 convolutional layer with kernel size 1x1\n",
    "\n",
    "All convolutional layers are with strides 1x1 (default), zero padding ('same'), and ReLU ('relu') activation function. \n",
    "\n",
    "The max pooling layer uses also zero padding to keep the size of the activation volume.\n",
    "\n",
    "In the forward path, the results of the four paths are concatenated by the last dimension (axis). You can use the `concat()` function of TensorFlow, which takes a Python list of tensors (the four variables that store the outputs of the four paths),\n",
    "and you need to specify the last axis, e.g. by using `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eecf5115-d522-4135-b6c5-73230396182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, concatenate\n",
    "\n",
    "class InceptionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.path1 = Conv2D(filters[0], (1, 1), activation='relu', padding='same')\n",
    "\n",
    "        self.path2 = Conv2D(filters[1], (1, 1), activation='relu', padding='same')\n",
    "        self.path2_2 = Conv2D(filters[2], (3, 3), activation='relu', padding='same')\n",
    "\n",
    "        self.path3 = Conv2D(filters[3], (1, 1), activation='relu', padding='same')\n",
    "        self.path3_2 = Conv2D(filters[4], (5, 5), activation='relu', padding='same')\n",
    "\n",
    "        self.path4 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')\n",
    "        self.path4_2 = Conv2D(filters[5], (1, 1), activation='relu', padding='same')\n",
    "\n",
    "    def call(self, x):\n",
    "        p1 = self.path1(x)\n",
    "        p2 = self.path2(x)\n",
    "        p2 = self.path2_2(p2)\n",
    "        p3 = self.path3(x)\n",
    "        p3 = self.path3_2(p3)\n",
    "        p4 = self.path4(x)\n",
    "        p4 = self.path4_2(p4)\n",
    "\n",
    "        # Concatenate along the last axis\n",
    "        return concatenate([p1, p2, p3, p4], axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7142481f-7e79-4145-972e-368b1cd27dde",
   "metadata": {},
   "source": [
    "The GoogLeNet architecture is then defined as follows:\n",
    "\n",
    "**Stump:**\n",
    "- Input layer as in the previous two networks\n",
    "- Convolutional layer with 64 filters of size (7x7) with strides (2x2)\n",
    "- Max pooling layer (3x3) with strides (2x2)\n",
    "- There comes a local response normalization at this point that you can skip for this notebook\n",
    "- Convolutional layer with 64 filters of size (1x1) with strides (1x1)\n",
    "- Convolutional layer with 192 filters of size (3x3) with strides (1x1)\n",
    "- Again a local response normalization that you can skip\n",
    "- Max pooling layer (3x3) with strides (2x2)\n",
    "\n",
    "**Main body:**\n",
    "- InceptionBlock(filters=[64, 96, 128, 12, 32, 32])\n",
    "- InceptionBlock(filters=[128, 128, 192, 32, 96, 64])\n",
    "- Max pooling layer (3x3) with strides (2x2)\n",
    "- InceptionBlock(filters=[192, 96, 208, 16, 48, 64])\n",
    "- InceptionBlock(filters=[160, 112, 224, 24, 64, 64])\n",
    "- InceptionBlock(filters=[128, 128, 256, 24, 64, 64])\n",
    "- InceptionBlock(filters=[112, 144, 288, 32, 64, 64])\n",
    "- InceptionBlock(filters=[256, 160, 320, 32, 128, 128])\n",
    "- Max pooling layer (3x3) with strides (2x2)\n",
    "- InceptionBlock(filters=[256, 160, 320, 32, 128, 128])\n",
    "- InceptionBlock(filters=[384, 192, 384, 48, 128, 128])\n",
    "\n",
    "**The filters of the Inception modules are given as:**\n",
    "- filters[0] -> only convolutional layer of 1st path\n",
    "- filters[1] -> 1st convolutional layer (1x1) of 2nd path\n",
    "- filters[2] -> 2nd convolutional layer (3x3) of 2nd path\n",
    "- filters[3] -> 1st convolutional layer (1x1) of 3rd path\n",
    "- filters[4] -> 2nd convolutional layer (5x5) of 3rd path\n",
    "- filters[5] -> only convolutional layer (1x1) of 4th path\n",
    "\n",
    "**Classifier head:**\n",
    "- Global average pooling (as in ResNet)\n",
    "- A dropout (`Dropout`) layer with a dropout rate of 0.4 \n",
    "- A fully connected layer that outputs the probabilities for ten classes \n",
    "\n",
    "**Ignore the auxiliary heads.**\n",
    "\n",
    "All layers have zero padding, and use the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebe3938c-97c7-421b-ad90-fdc723b3e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D , GlobalAveragePooling2D ,Dropout,Dense,concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def stump(input_layer):\n",
    "    x = Conv2D(64, (7, 7), activation='relu', strides=(2, 2), padding='same')(input_layer)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = Conv2D(64, (1, 1), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    return x\n",
    "\n",
    "def Inception_Block(x, filters):\n",
    "    inception_block = InceptionBlock(filters)\n",
    "    x = inception_block(x)\n",
    "    return x\n",
    "\n",
    "# Input layer\n",
    "input_shape = (224, 224, 3)\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Stump\n",
    "x = stump(input_layer)\n",
    "\n",
    "# Main body with Inception blocks\n",
    "x = Inception_Block(x, filters=[64, 96, 128, 12, 32, 32])\n",
    "x = Inception_Block(x, filters=[128, 128, 192, 32, 96, 64])\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = Inception_Block(x, filters=[192, 96, 208, 16, 48, 64])\n",
    "x = Inception_Block(x, filters=[160, 112, 224, 24, 64, 64])\n",
    "x = Inception_Block(x, filters=[128, 128, 256, 24, 64, 64])\n",
    "x = Inception_Block(x, filters=[112, 144, 288, 32, 64, 64])\n",
    "x = Inception_Block(x, filters=[256, 160, 320, 32, 128, 128])\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = Inception_Block(x, filters=[256, 160, 320, 32, 128, 128])\n",
    "x = Inception_Block(x, filters=[384, 192, 384, 48, 128, 128])\n",
    "\n",
    "# Classifier head\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "output_layer = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c2d183-5d49-4553-a97a-769e7d7083fa",
   "metadata": {},
   "source": [
    "In the summary, you can now compare if the general structure of your network fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "492c574f-08c8-40e2-91cc-8ee48c315bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_288 (Conv2D)         (None, 112, 112, 64)      9472      \n",
      "                                                                 \n",
      " max_pooling2d_67 (MaxPoolin  (None, 56, 56, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_289 (Conv2D)         (None, 56, 56, 64)        4160      \n",
      "                                                                 \n",
      " conv2d_290 (Conv2D)         (None, 56, 56, 192)       110784    \n",
      "                                                                 \n",
      " max_pooling2d_68 (MaxPoolin  (None, 28, 28, 192)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " inception_block_45 (Incepti  (None, 28, 28, 256)      159724    \n",
      " onBlock)                                                        \n",
      "                                                                 \n",
      " inception_block_46 (Incepti  (None, 28, 28, 480)      388736    \n",
      " onBlock)                                                        \n",
      "                                                                 \n",
      " max_pooling2d_71 (MaxPoolin  (None, 14, 14, 480)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " inception_block_47 (Incepti  (None, 14, 14, 512)      376176    \n",
      " onBlock)                                                        \n",
      "                                                                 \n",
      " inception_block_48 (Incepti  (None, 14, 14, 512)      449160    \n",
      " onBlock)                                                        \n",
      "                                                                 \n",
      " inception_block_49 (Incepti  (None, 14, 14, 512)      510104    \n",
      " onBlock)                                                        \n",
      "                                                                 \n",
      " inception_block_50 (Incepti  (None, 14, 14, 528)      605376    \n",
      " onBlock)                                                        \n",
      "                                                                 \n",
      " inception_block_51 (Incepti  (None, 14, 14, 832)      868352    \n",
      " onBlock)                                                        \n",
      "                                                                 \n",
      " max_pooling2d_77 (MaxPoolin  (None, 7, 7, 832)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " inception_block_52 (Incepti  (None, 7, 7, 832)        1043456   \n",
      " onBlock)                                                        \n",
      "                                                                 \n",
      " inception_block_53 (Incepti  (None, 7, 7, 1024)       1444080   \n",
      " onBlock)                                                        \n",
      "                                                                 \n",
      " global_average_pooling2d_4   (None, 1024)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,979,830\n",
      "Trainable params: 5,979,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d09b4d-df35-4f09-9621-4df068f70d78",
   "metadata": {},
   "source": [
    "**Question 3: How many trainable layers does GoogLeNet have?**\n",
    "\n",
    "**Question 4: Why do ResNet and GoogLeNet have so few trainable parameters in comparison to VGG?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893082a-f972-4430-9040-c10b5d9bafe7",
   "metadata": {},
   "source": [
    "# Answers to questions:\n",
    "\n",
    "**Question 1: Does the constructed ResNet34 model really contain 34 layers as the name suggests?**  \n",
    "First, we count only trainable layers and not the remaining layers that have no weights (like ReLU, batch nofrmalization, pooling, etc.). Second, we only count the layers along the longest path. If there are parallel paths, as in ResNet, then only the layers in the longer path are counted, and the ones in the shorter path (like skip connection path in ResNet) are not counted. In the given example of ResNet34, there is one trainable (Conv2D) layer in the stump, and one trainable (Dense) layer in the classifier. The main path of each residual block contains two trainable (Conv2D) layers, and one trainable (Conv2D) layer in the shorter skip connection layer. Since we have 16 residual blocks, there are altogether 32 trainable layers. Together this makes 34 trainable layers.\n",
    "\n",
    "**Question 2: What do you need to change to get from the VGG16 to the VGG 19 architecture?**  \n",
    "The only difference is that the VGG blocks 3, 4, and 5 have four convolutional layers instead of three.\n",
    "\n",
    "**Question 3: How many trainable layers does GoogLeNet have?**  \n",
    "The stump has three convolutional layers, there are nine Inception blocks with two trainable layers each, and one dense layer in the classifier. Altogether that makes 22 trainable layers.\n",
    "\n",
    "**Question 4: Why do ResNet and GoogLeNet have so few trainable parameters in comparison to VGG?**  \n",
    "Most of the trainable parameters in VGG are in the first fully connected (dense) layer that has 4,096 units that all take 25,088 values as inputs. Together with the bias, that makes 4,096\\*25,089=102,764,544 trainable parameters, which is already around five times as many parameters as ResNet and GoogLeNet have in total. The second fully connected (dense) layer in VGG has another 16,781,312 trainable parameters, which is also quite a lot. ResNet and GoogLeNet also use global average pooling to reduce the activation volumes of the main convolutional body to a small vector of 512 and 1024 values, which taken as input to the subsequence fully connected (dense) layer results in much less trainable parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
