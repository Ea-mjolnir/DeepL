{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfeac516-b544-48c1-8f1f-7dc6123ecf3a",
   "metadata": {},
   "source": [
    "# Exercise 06 - Gradient Computations\n",
    "\n",
    "This notebook is about gradient computations in neural networks, and how to get the weights and gradient vector for the model with TensorFlow. It also considers the gradient descent step, and performs some steps manually and explores the development of the neural network outputs.\n",
    "\n",
    "**Learning objectives:**\n",
    "- Get to know gradient calculations\n",
    "- Learn to work with the gradient tape of TensorFlow\n",
    "- See how to perform user defined weight initialization\n",
    "- Manually do the computations from output scores to class probabilities, and then to a loss value\n",
    "- Predict class index from scores or probabilities\n",
    "- Get gradients from intermediate results\n",
    "\n",
    "Before you start, find a GPU on the system that is not heavily used by other users (with **nvidia-smi**), and change X to the id of this GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14a64d48-b7c5-4650-802f-3c040b61b543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change X to the GPU number you want to use,\n",
    "# otherwise you will get a Python error\n",
    "# e.g. USE_GPU = 4\n",
    "USE_GPU = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8f16c5b-da07-4ea9-9918-e86daf3740c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "\n",
      "Available GPU Devices:\n",
      "  /physical_device:GPU:0 GPU\n",
      "  /physical_device:GPU:1 GPU\n",
      "  /physical_device:GPU:2 GPU\n",
      "  /physical_device:GPU:3 GPU\n",
      "  /physical_device:GPU:4 GPU\n",
      "  /physical_device:GPU:5 GPU\n",
      "  /physical_device:GPU:6 GPU\n",
      "  /physical_device:GPU:7 GPU\n",
      "\n",
      "Visible GPU Devices:\n",
      "  /physical_device:GPU:4 GPU\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow \n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the installed TensorFlow version\n",
    "print(f'TensorFlow version: {tf.__version__}\\n')\n",
    "\n",
    "# Get all GPU devices on this server\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all GPU devices\n",
    "print('Available GPU Devices:')\n",
    "for gpu in gpu_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set only the GPU specified as USE_GPU to be visible\n",
    "tf.config.set_visible_devices(gpu_devices[USE_GPU], 'GPU')\n",
    "\n",
    "# Get all visible GPU  devices on this server\n",
    "visible_devices = tf.config.get_visible_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all visible GPU devices\n",
    "print('\\nVisible GPU Devices:')\n",
    "for gpu in visible_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set the visible device(s) to not allocate all available memory at once,\n",
    "# but rather let the memory grow whenever needed\n",
    "for gpu in visible_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e27a3-c67b-4a8c-9d5a-6488ef0b54ee",
   "metadata": {},
   "source": [
    "## Neuron with sigmoid activation\n",
    "\n",
    "In the first part, we look at gradient computation using a neuron with two input values and one output, like the one used in the lecture to illustrate the backpropagation algorithm. For this purpose, a sequential model is constructed with one dense (fully connected) layer consisting of exactly one neuron. \n",
    "\n",
    "The layer weights of neural networks can be initialized in TensorFlow with initialization functions or initialization classes. By default, the initializer for the layer weights (the `kernel_initializer`) is the Xavier (or Glorot) uniform initializer, and the initializer for the biases (the `bias_initializer`) is the Zeroes initializer that initializes all values with zero. But there are other initializers implemented in TensorFlow that could be used or you can implement an function or class for this purpose. In order to get the weights initialized according to the example of the lecture, we define a function `init_with_specific_values()` that returns a tensor with values 2.0 and -3.0 for the two weights. (There is no initializer that we could use that provides specific values, since this is not how we typically want to initialize our neurons. Rather, we want to initialize with small random values according to some distribution.) An initializer function must take the shape and optionally the data type as arguments and return the tensor accordingly. To explore what the expected shape is, we print the shape argument in the function. TensorFlow works very well together with NumPy, so that we can return the initial weight values per NumPy array and TensorFlow will convert it into a TensorFlow tensor object. The alternative would be to directly construct a TensorFlow Tensor object. For initializing the bias, we can use the `Constant` initializer class from TensorFlow (Keras) that we construct in a way that it always returns the value -3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edceecb8-5001-4dbe-8d6e-942785a6f802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel initializer should return tensor of shape: [2, 1] \n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " DenseLayer (Dense)          (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def init_with_specified_values(shape, dtype=None):\n",
    "    print('Kernel initializer should return tensor of shape:', shape, '\\n')\n",
    "    return np.array([[2.0], [-3.0]])\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_shape=(2,),\n",
    "          units = 1, \n",
    "          kernel_initializer=init_with_specified_values, #usually by default : 'glorot_uniform'\n",
    "          bias_initializer=tf.keras.initializers.Constant(value=-3),\n",
    "          activation = 'sigmoid',\n",
    "          name='DenseLayer')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a35a98f-bd96-4ca6-abb5-03ccff519fa6",
   "metadata": {},
   "source": [
    "The tensor shape is expected to be (2,1), which means we need a tensor with two rows and one column (one value per row). We therefore have to put each value in brackets, and have another set of brackets for defining the one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9963951-ea6f-4481-a316-250a7f1934c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[2.0], [-3.0]]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0741cca-d743-48b5-acb7-31c4bb91d5b0",
   "metadata": {},
   "source": [
    "If we would leave out the inner brackets, we would end up with a tensor of shape (2,), which would be a vector of two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d1069b2-7611-4432-b08d-9c83f1886fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([2.0, -3.0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405dc87-ee6f-407b-9b02-578591f795f9",
   "metadata": {},
   "source": [
    "Before we continue on: what would be the tensor shape that is expected from the initializer function, if the dense layer has three neurons instead of one? Think about it, try it out to have the tensor shape printed, and construct an array accordingly that provides a tensor of this shape with your own values.\n",
    "\n",
    "**Task: In the following, the ´zeros()´ function of NumPy is used to construct a tensor of zeroes in the specified shape. Replace it with the `array()` function to construct a tensor of the same shape, but with your own values (e.g. 1.0, 2.0, 3.0, ...).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e30e3ea-04a3-44fc-a642-32dd0b073227",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel initializer should return tensor of shape: [2, 3] \n",
      "\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " DenseLayer (Dense)          (None, 3)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def init_with_specified_values(shape, dtype=None):\n",
    "    print('Kernel initializer should return tensor of shape:', shape, '\\n')\n",
    "    return np.zeros(shape)\n",
    "#    return np.array([...])\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_shape=(2,),\n",
    "          units = 3, \n",
    "          kernel_initializer=init_with_specified_values,  # usually by default : 'glorot_uniform'\n",
    "          bias_initializer=tf.keras.initializers.Constant(value=-3),\n",
    "          activation = 'sigmoid',\n",
    "          name='DenseLayer')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33994175-d693-4cbb-95c4-24ee70bff3c4",
   "metadata": {},
   "source": [
    "For the sake of completeness, a version without NumPy is also shown below, which generates a TensorFlow tensor instead in the initialization function. TensorFlow distinguishes tensors that are constant (non-trainable) or are variable (trainable). Since the tensor is used for trainable weights, the tensor is generated with the `Variable()` constructor. (We could also use `constant()` to generate a non-trainable tensor, and TensorFlow makes a trainable copy of this constant in the background. Try out the commented out line that uses the `constant()` function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "860a70d9-7704-416a-8d3d-ac668aec90c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " DenseLayer (Dense)          (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def init_with_specified_values(shape, dtype=None):\n",
    "    return tf.Variable([[2.0], [-3.0]])\n",
    "#    return tf.constant([[2.0], [-3.0]])\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_shape=(2,),\n",
    "          units = 1, \n",
    "          kernel_initializer=init_with_specified_values, \n",
    "          bias_initializer=tf.keras.initializers.Constant(value=-3),\n",
    "          activation = 'sigmoid',\n",
    "          name='DenseLayer')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915a70b-3357-45d7-a6c5-9b0fb8f8eee0",
   "metadata": {},
   "source": [
    "The trainable weights of a model can be accessed with the `trainable_weights` attribute. The attribute is iterable, and we can access all elements with a foor loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fe77635-6db0-495d-af1e-1126272bcab9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'DenseLayer/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[ 2.],\n",
      "       [-3.]], dtype=float32)>\n",
      "<tf.Variable 'DenseLayer/bias:0' shape=(1,) dtype=float32, numpy=array([-3.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "for v in model.trainable_weights:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de669a02-4200-46ae-babf-d2fd6d6e2d48",
   "metadata": {},
   "source": [
    "To finish up on our example, we construct an input tensor for the model with input values -1.0 and -2.0. Here, a version using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc223f0b-8cfc-4a7e-8c87-c1106bab9ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "[-1. -2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([-1.0, -2.0])\n",
    "\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc627f5-bfe0-4e04-8089-c65fb034e567",
   "metadata": {},
   "source": [
    "However, we cannot use the tensor in this way, since neural network models expect batches of data. So, we have to add another (first) dimension to the tensor that denotes the batch size. Since we only use a single dataset as a batch, the batch size is one. Therefore, we reshape the tensor of two values to a tensor of shape (1,2). Now we have a batch of one dataset, where each dataset has 2 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64ffb559-c640-4883-80e2-1ef7d7a57f97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape((1, 2))\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9eab3-f1d8-47b1-9a9a-2970c2edd750",
   "metadata": {},
   "source": [
    "Again, the TensorFlow version constructing a Tensor object. Check out the brackets to directly generate a tensor of shape (1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffc6488f-3b41-497a-bdf8-4dabdd2964c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-1. -2.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[-1.0, -2.0]])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654a926-132f-4199-8f97-9d13875b63f6",
   "metadata": {},
   "source": [
    "Finally, we can use the input tensor x in the forward pass by calling the neural network model object with the input data. In Python, we can call objects of a class like a function, which is equivalent like calling the method `__call__()` of the object. For TensorFlow classes that are derived from the class `Layer`, this `__call__()` method performs a forward pass and does all the necessary calculations. The result is the predicted target probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba7f533d-fddb-4fac-9779-973fd5fe1608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.8021838]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(x)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368beb90-c652-42db-bd51-2b7e541a89ca",
   "metadata": {},
   "source": [
    "The value of the returned tensor is around 0.73, which means the neuron (the network) is 73% sure that the object of the input belongs to the positive class.\n",
    "\n",
    "When we call the model or layer methods within the context of the `GradientTape()`, then the operations that are called within this context are recorded and can be used for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b8385da-642c-46db-b718-cf7fd9ebe9b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7eb741-7357-44f2-b384-b9d6be3f6c50",
   "metadata": {},
   "source": [
    "Calling the method `gradient` of the gradient tape on source (first argument) and target (second argument), the list of target tensors like the trainable weights are differentiated against the elements of the source (first argument). Here, the target are the trainable weights of the model that are differentiated against y. (We basically calculate the partial derivatives of the function that calculated y with respect to all weights of the model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60e19811-1604-48e9-ad2b-874c76cc9d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grad = tape.gradient(y, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67e2f73-7f8c-4baf-8eca-a32615631320",
   "metadata": {},
   "source": [
    "Since both the number of trainable weights are the same as the number of elements in the gradient object, we can zip them together in order to be able to iterate over them at the same time. We use this to print all gradients with the name of the variable. Since the elements of the gradient object can be scalar or vectors, we also iterate (within the first loop) over the gradient vectors and print the elements of these vectors row by row. (We could leave out the second loop and print g instead of looping through its elements, but then the output will not be printed as nicely row by row.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ffe009db-fd6f-43c3-bd5d-db39cd228c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseLayer/kernel:0  [-0.22878425]\n",
      "DenseLayer/kernel:0  [-0.4575685]\n",
      "DenseLayer/bias:0    0.2287842482328415\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(model.trainable_weights, grad):\n",
    "    for i in g:\n",
    "        print(f'{var.name:<20} {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e9b70c-4d8d-4c59-b9a6-5d7e67905d17",
   "metadata": {},
   "source": [
    "As you notice, the gradients are (almost) the same as in the lecture. The small differences come from the fact that the numbers in the lecture slides were rounded to the second decimal, and the small rounding errors accumulated up to the gradients of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e5703-f893-4680-82ff-e41cafa389de",
   "metadata": {},
   "source": [
    "## Weights that result in higher score\n",
    "\n",
    "Next, we use initial weights that give us a slightly higher score and probability than before. For example, after one or more training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17c31169-955f-4d33-b3ac-880a796b59dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.8021838]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def init_with_specified_values(shape, dtype=None):\n",
    "    return tf.Variable([[1.9], [-3.1]])\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_shape=(2,),\n",
    "          units = 1, \n",
    "          kernel_initializer=init_with_specified_values, \n",
    "          bias_initializer=tf.keras.initializers.Constant(value=-2.9),\n",
    "          activation = 'sigmoid',\n",
    "          name='DenseLayer')\n",
    "])\n",
    "\n",
    "y_pred = model(x)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48d298-4fed-4d31-974e-4e29cf0a2631",
   "metadata": {},
   "source": [
    "The predicted value is now a probability of 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff233770-ec6c-4800-808a-602f15c673f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseLayer/kernel:0  [-0.15868495]\n",
      "DenseLayer/kernel:0  [-0.3173699]\n",
      "DenseLayer/bias:0    0.15868495404720306\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = model(x)\n",
    "\n",
    "grad = tape.gradient(y, model.trainable_weights)\n",
    "\n",
    "for var, g in zip(model.trainable_weights, grad):\n",
    "    for i in g:\n",
    "        print(f'{var.name:<20} {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef43a4-1e99-42a8-93db-83be76de84ae",
   "metadata": {},
   "source": [
    "You notice that the gradients (the gradient vector) points to the same direction, but it is shorter (has smaller values). This is because the score (class probability) is higher and the slope of the sigmoid function smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfa620-29c3-4d8c-adb8-842fe1ed2e1c",
   "metadata": {},
   "source": [
    "## Weights that result in lower score\n",
    "\n",
    "And now changing the weights to get a slighly smaller score and probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10f909cb-e287-4e29-b0d9-981759cbf395",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.6456564]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def init_with_specified_values(shape, dtype=None):\n",
    "    return tf.Variable([[2.1], [-2.9]])\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_shape=(2,),\n",
    "          units = 1, \n",
    "          kernel_initializer=init_with_specified_values, \n",
    "          bias_initializer=tf.keras.initializers.Constant(value=-3.1),\n",
    "          activation = 'sigmoid',\n",
    "          name='DenseLayer')\n",
    "])\n",
    "\n",
    "y_pred = model(x)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8ee96-9c56-43b9-8ace-917abd268741",
   "metadata": {},
   "source": [
    "The predicted value is now a probability of 64.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6be1c978-bce6-4c6e-a6d3-deaabe2ca274",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseLayer/kernel:0  [-0.22878422]\n",
      "DenseLayer/kernel:0  [-0.45756844]\n",
      "DenseLayer/bias:0    0.2287842184305191\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = model(x)\n",
    "\n",
    "grad = tape.gradient(y, model.trainable_weights)\n",
    "\n",
    "for var, g in zip(model.trainable_weights, grad):\n",
    "    for i in g:\n",
    "        print(f'{var.name:<20} {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d701801-73cd-47d2-8fd8-f7a33a43710b",
   "metadata": {},
   "source": [
    "And the gradient vector is now longer.\n",
    "\n",
    "If we play around with the weights until we have a predicted probability of around 1.0 - 0.645 = 0.355 (where 0.645 is the above probability), we end up with the same gradient vector. (You need to ignore the slight differences that result from the probabilities not being exactly opposite.) The reason is that the sigmoid function is symmetric and we get the same gradients for y and 1.0-y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "445ae67b-26a1-4c1f-9ecd-4011397d89de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.35434368]], shape=(1, 1), dtype=float32) \n",
      "\n",
      "DenseLayer/kernel:0  [-0.22878425]\n",
      "DenseLayer/kernel:0  [-0.4575685]\n",
      "DenseLayer/bias:0    0.2287842482328415\n"
     ]
    }
   ],
   "source": [
    "def init_with_specified_values(shape, dtype=None):\n",
    "    return tf.Variable([[2.45], [-2.55]])\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_shape=(2,),\n",
    "          units = 1, \n",
    "          kernel_initializer=init_with_specified_values, \n",
    "          bias_initializer=tf.keras.initializers.Constant(value=-3.25),\n",
    "          activation = 'sigmoid',\n",
    "          name='DenseLayer')\n",
    "])\n",
    "\n",
    "y_pred = model(x)\n",
    "\n",
    "print(y_pred, \"\\n\")\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = model(x)\n",
    "\n",
    "grad = tape.gradient(y, model.trainable_weights)\n",
    "\n",
    "for var, g in zip(model.trainable_weights, grad):\n",
    "    for i in g:\n",
    "        print(f'{var.name:<20} {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07254dba-7d19-4bbe-9e63-b2fb54eb8b94",
   "metadata": {},
   "source": [
    "## 2-layer dense network with softmax loss\n",
    "\n",
    "In the next part, we construct a larger network with two dense layers, both with five neurons. We initialize their weights with the `RandomNormal()` initialier with zero mean and 0.05 as standard deviation. To reproduce the results, a seed value of 7 is used to generate random numbers. Now we have five scores as output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5431535f-133d-4cfc-9405-078489e7b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(input_shape=(2,),\n",
    "          units = 5, \n",
    "          kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=7), \n",
    "          bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=7),\n",
    "          activation = 'sigmoid',\n",
    "          name='DenseLayer1'),\n",
    "    Dense(units = 5, \n",
    "          kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=7), \n",
    "          bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=7),\n",
    "          name='DenseLayer2')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6b44d-40a9-4d80-b442-1a53bb299ac2",
   "metadata": {},
   "source": [
    "As input, we construct a tensor with random numbers. (We specify the seed value of the random function to always get the same values to reproduce our results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea27cd-4cbb-4859-a408-f85b8a047465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.random import uniform\n",
    "\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "x = tf.random.uniform((1,2))\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350c7b4-b420-4abc-b98d-143bade823bb",
   "metadata": {},
   "source": [
    "Applying the model on the input data x, we get the predicted class scores y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f81bfa-539f-4c2d-b4d7-4c6370b45507",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a57e4f-a2d6-4b3a-b160-c5548d3949ed",
   "metadata": {},
   "source": [
    "To get from the scores to the probabilities, the softmax function can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69074b7-3d7b-48fc-8b0e-ef6a02380e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.nn import softmax\n",
    "\n",
    "y_prob = softmax(y_pred)\n",
    "\n",
    "y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f05ee-f301-4042-ae7a-1cebc5db8b6c",
   "metadata": {},
   "source": [
    "You notice that the probability of the class with index 1 is the highest and we would predict class 1.\n",
    "\n",
    "We could also use the `argmax()` function on the tensor of probabilities (or on the tensor of scores) to get the predicted class index. Since the tensor is of shape (1,5), we need to apply the argmax function on the second (the last) dimension, by specifying the axis to be 2 (or -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab38d2e6-3eab-4952-a2a4-0ae8ebadf451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.math import argmax\n",
    "\n",
    "cls_label = tf.math.argmax(y_prob, axis=-1)\n",
    "\n",
    "print(cls_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3c70c-aee2-4a28-afc4-834bdb1068ea",
   "metadata": {},
   "source": [
    "Remember that we typically have the correct class label to start with (now stored in cls_label). With the true class label and the predicted scores, we can compute the sparse softmax cross entropy loss with the `sparse_softmax_cross_entropy_with_logits()` function. Sparse mean that we have the class labels as integer values instead of a one-hot encoding vector. And from logits means that we have not applied the softmax yet, but input the predicted scores to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd6482-2bb1-42d2-b8d4-e5c419aadb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.nn import sparse_softmax_cross_entropy_with_logits\n",
    "\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=cls_label, logits=y_pred)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb81794-5def-45b5-9626-379ae8c975e2",
   "metadata": {},
   "source": [
    "The loss value for the above configuration is 1.5745.\n",
    "\n",
    "From our above probabilities, we can slice out the probability of the true class, and apply the negative log on it to get the loss by hand. We need to use the slice operator on the `y_prob` tensor, since it is of shape (1,5), and we want to get the value from the first row (first index 0), and from the second column (which would be index 1, the true class). Since `cls_label` is a one-dimensional tensor, we need to use the index operator to get the value out. Hence the somewhat cumbersome syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951213f8-e483-4283-83c0-528f0397c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.math import log\n",
    "\n",
    "y_prob_for_true_cls = y_prob[0, cls_label[0]]\n",
    "\n",
    "-log(y_prob_for_true_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716b53f-0588-4ea1-af39-b4912141a071",
   "metadata": {},
   "source": [
    "But in the end, we get the same loss value of 1.5745. So, our calculations by hand were correct.\n",
    "\n",
    "Let's put this all together and apply it in the context of the gradient tape. We first apply the model on input x to get intermediate outputs y, and then use y as the input to the loss function. We then compute the gradients of the trainable weights of the model with respect to the loss value, and print them all out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c9a9ad-993e-4354-9cc2-da4074cd9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = model(x)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=cls_label, logits=y)    \n",
    "\n",
    "grad = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "for var, g in zip(model.trainable_weights, grad):\n",
    "    for i in g:\n",
    "        print(f'{var.name:<20} {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab3148-40c0-451d-9534-9300d213c070",
   "metadata": {},
   "source": [
    "Assuming that the correct (the true) label is not class 1, but class 2, which is actually the lowest score, we get the following loss value and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4f19c-6062-4051-8850-326a0199ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = model(x)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.Variable([2]), logits=y) \n",
    "                                                          \n",
    "print('Loss:', loss, '\\n')\n",
    "\n",
    "grad = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "for var, g in zip(model.trainable_weights, grad):\n",
    "    for i in g:\n",
    "        print(f'{var.name:<20} {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4efa22-85e7-4574-9aee-b6c87153336e",
   "metadata": {},
   "source": [
    "The loss with value 1.69 is higher than the previous one, which is correct since the probability of that class is lower, and the gradients of the weights are also different. If we apply this gradient vector to the weights, the loss value with class 2 as the true class should improve, as well as the probability. Let's try this out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338a52e-b39c-4963-b086-b43a9633724f",
   "metadata": {},
   "source": [
    "## Manual gradient descent step\n",
    "\n",
    "In order to make a gradient descent step, we use an optimizer object. The most simple one is stochastic gradient descent, as it will only subtract the gradient vector (multiplied by the learning rate) from the weights. Therefore, we first need to construct such an optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b01be-cf21-4f3b-8013-cfa8cd1e4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b044198-f20c-4298-bf40-c3dcf68c1a2e",
   "metadata": {},
   "source": [
    "Before we make the gradient descent step, let us output the weights of the model. But only from the first layer and without the biases, as otherwise the output would be too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3764b47-187f-4bf9-8cee-b06aa22226f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable_weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f85318-3ef1-4a5a-8e58-c4efb5974be4",
   "metadata": {},
   "source": [
    "And we output the gradients of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d9034-4539-4ba3-8dd8-afe23cd15dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2943d-c455-44a3-89c8-ba26c164ad2d",
   "metadata": {},
   "source": [
    "A gradient descent step by hand would subtract from the gradient, which is multiplied by the learning rate, from the trainable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c130a-96a7-4355-8585-c4dee3e1f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable_weights[0] - 0.001 * grad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9661e-7590-4624-a943-d3f1beccd7e8",
   "metadata": {},
   "source": [
    "Using the above constructed optimizer object, we can call the `apply_gradients()` function, which receives an iterable object that returns pairs of gradients and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74056779-d06f-497c-96f4-62e1d9426b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.apply_gradients(zip(grad, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba86a0-7dcd-4c83-80c5-984f475f4ac6",
   "metadata": {},
   "source": [
    "After the gradient descent step, the weights of the model are updated, and are the same as the ones we calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487d4f8-1742-4999-9d7d-ccf3d85b4b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable_weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c24c77-421f-43fd-a567-d02f5f167c8c",
   "metadata": {},
   "source": [
    "If we input the same data as before to the updated model, we should get a lower loss value as before, and also the probabilities should change so that our class 2 (that we assume to be the true class) is improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848594f8-eea7-4150-9288-9d233dc94c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x)\n",
    "\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.Variable([2]), logits=y) \n",
    "\n",
    "print(loss)\n",
    "\n",
    "y_prob_new = softmax(y)\n",
    "\n",
    "print('Probabilities before update:', y_prob)\n",
    "print('Probabilities after update :', y_prob_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c458e-1555-496d-95fa-903527ab76fa",
   "metadata": {},
   "source": [
    "And this is actually the case: The loss decreased a little from 1.6985874 to 1.6967251, and the probabilities changed slightly in favor of class 2.\n",
    "\n",
    "We can do this gradient descent step another 100 times by having the above code of performing the forward pass in the context of the gradient tape, calculating the gradients, and applying the gradients to the trainable weights of the model, into a for loop that is executed 100 times. (The loss value is printed just once before the loop and once more after the loop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1da22-0a50-4852-964b-a608549daf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loss:', loss)\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = model(x)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.Variable([2]), logits=y) \n",
    "        \n",
    "    grad = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_weights))\n",
    "    \n",
    "print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6bef6b-cea1-483c-a8c9-afbad6b392c0",
   "metadata": {},
   "source": [
    "Once more, we do a prediction with the input data, show the loss, and how the predicted probabilities changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a02551-ca3f-4584-968d-fa7624c9353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x)\n",
    "\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.Variable([2]), logits=y) \n",
    "\n",
    "print(loss)\n",
    "\n",
    "y_prob_new = softmax(y)\n",
    "\n",
    "print('Probabilities before update:', y_prob)\n",
    "print('Probabilities after update :', y_prob_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f189b4c-c810-4c1f-b083-d64cdde92279",
   "metadata": {},
   "source": [
    "The loss function is now much lower than what we started with, and the probabilities are now that the model would predict class 2.\n",
    "\n",
    "**But please keep in mind that we trained this model with one dataset only. Normally, we would train a model with lots of datasets and with many different true target classes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5878daa4-52ac-4f32-8ce2-575183e6081e",
   "metadata": {},
   "source": [
    "## Gradients of intermediate results\n",
    "\n",
    "With the gradient tape, we can also calculate the gradients of intermediate results. In the following, we calculate the gradients for both y, which is the result of the model taking input x, as well as all the trainable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac149dbf-b6a3-4654-8dc7-e89886c64f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = model(x)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.Variable([2]), logits=y)\n",
    "    \n",
    "grad = tape.gradient(loss, [y, model.trainable_weights])\n",
    "\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc81a05a-380b-474b-b489-88181602c764",
   "metadata": {},
   "source": [
    "The first output tensor is the gradient of y. Remember that y is not part of the trainable weights of the model, it is the score vector from the model before these scores go into the loss function.\n",
    "\n",
    "Last, we do some basic mathematical computations (log(c*(a+b))), where we store the result in a tensor variable, and compute and print the gradients of this computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f71bef-40d8-476d-bbfa-90344c415e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable((5.0))\n",
    "b = tf.Variable((7.3))\n",
    "c = tf.Variable((0.2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x = a + b\n",
    "    y = x * c\n",
    "    z = log(y)\n",
    "\n",
    "grad = tape.gradient(z, [x, y, z])\n",
    "\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa205d-91ba-4e1d-812a-6253ea5cab94",
   "metadata": {},
   "source": [
    "Notice that the gradient of z (the final output) takes the value 1.0. \n",
    "\n",
    "In the shown way, using the TensorFlow functions on TensorFlow tensors, a computational graph is build in the background, it is evaluated in a forward pass, and by using the `gradiend()` method of the gradient tape object, we can get the gradients that result from backpropagation. Then, the gradients can be applied on the trainable weights of the tensors. In this way, we could define our own training loop. However, the `fit()` method of the TensorFlow model class is much more convenient, and we have a lot of ways to configure our training loop. For example, use different optimizers that not just subtracts the gradient vector from the weights, but also builds and keeps track of some momentum that allows to get to the global minimum of our function much more efficiently.\n",
    "\n",
    "If we do need more control over the training process, there are plenty of ways to define our own cost functions, weight initializers, optimizers, etc. by deriving from existing TensorFlow (base) classes, and use those in the training process with the fit method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
