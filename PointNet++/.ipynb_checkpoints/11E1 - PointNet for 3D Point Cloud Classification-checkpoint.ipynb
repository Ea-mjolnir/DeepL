{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11.1 - PointNet for 3D Point Cloud Classification\n",
    "\n",
    "This exercises is about implementing the PointNet architecture for point-wise classification of a 3D point cloud by considering local neighborhoods of 20 points around each point to classify. (The point itself is also used, so that in the end there are 21 points as input to PointNet.)\n",
    "\n",
    "The datasets are provided as 3-dimensional tensors, where for each point of the point cloud (D), the 21 points (N=21) are given, with their 3D coordinates (Dx21x3). The training dataset of ISPRS is used for training, and the evaluation dataset for testing (predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change X to the GPU number you want to use,\n",
    "# otherwise you will get a Python error\n",
    "# e.g. USE_GPU = 4\n",
    "USE_GPU = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 21:53:30.749412: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-31 21:53:31.597908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "\n",
      "Available GPU Devices:\n",
      "  /physical_device:GPU:0 GPU\n",
      "  /physical_device:GPU:1 GPU\n",
      "  /physical_device:GPU:2 GPU\n",
      "  /physical_device:GPU:3 GPU\n",
      "  /physical_device:GPU:4 GPU\n",
      "  /physical_device:GPU:5 GPU\n",
      "  /physical_device:GPU:6 GPU\n",
      "  /physical_device:GPU:7 GPU\n",
      "\n",
      "Visible GPU Devices:\n",
      "  /physical_device:GPU:4 GPU\n",
      "\n",
      "Keras version: 2.12.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow \n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the installed TensorFlow version\n",
    "print(f'TensorFlow version: {tf.__version__}\\n')\n",
    "\n",
    "# Get all GPU devices on this server\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all GPU devices\n",
    "print('Available GPU Devices:')\n",
    "for gpu in gpu_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set only the GPU specified as USE_GPU to be visible\n",
    "tf.config.set_visible_devices(gpu_devices[USE_GPU], 'GPU')\n",
    "\n",
    "# Get all visible GPU  devices on this server\n",
    "visible_devices = tf.config.get_visible_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all visible GPU devices\n",
    "print('\\nVisible GPU Devices:')\n",
    "for gpu in visible_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set the visible device(s) to not allocate all available memory at once,\n",
    "# but rather let the memory grow whenever needed\n",
    "for gpu in visible_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "# Import Keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print the installed Keras version\n",
    "print(f'\\nKeras version: {keras.__version__}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_colorized_point_cloud(xyz, y, filename):\n",
    "\n",
    "    color_map = np.array([\n",
    "        [255, 255, 125],\n",
    "        [  0, 255, 255],\n",
    "        [255, 255, 255],\n",
    "        [255, 255,   0],\n",
    "        [  0, 255, 125],\n",
    "        [  0,   0, 255],\n",
    "        [  0, 125, 255],\n",
    "        [125, 255,   0],\n",
    "        [  0, 255,   0]])\n",
    "    \n",
    "    u, inverses = np.unique(y, return_inverse=True)    \n",
    "    \n",
    "    colors = color_map[inverses]\n",
    "    \n",
    "    df = pd.DataFrame(xyz, columns=['x', 'y', 'z'])    \n",
    "\n",
    "    df['red'] = pd.Series(data=colors[:,0], name='red')\n",
    "    df['green'] = pd.Series(data=colors[:,1], name='green')\n",
    "    df['blue'] = pd.Series(data=colors[:,2], name='blue')\n",
    "    \n",
    "    df.to_csv(filename, index=False, header=False)\n",
    "    \n",
    "    print(f'Saved \"{filename}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data\n",
    "\n",
    "The dataset used in this exercise consists of a large number (753.876 for training) of very small point clouds with 21 points each. They are the result of taking each point from the original point cloud together with their 20 neighbor points. And they form the input to the PointNet neural network to predict the class for the one \"central\" point.\n",
    "\n",
    "The point clouds are centered by their \"central\" point, so that the x,y-coordinates of the central point is in the origin of the coordinates system. And the z-coordinate of this point is the elevation over the terrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "root_dir = str(Path.home()) + r'/coursematerial/GIS/ISPRS/PointNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read \"Vaihingen3D_Training_PointNet_XYZ.npy\" feature matrix of shape (753876, 21, 3)\n",
      "Successfully read \"Vaihingen3D_Training_PointNet_Labels.npy\" label vector of shape  (753876, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "filename_xyz = 'Vaihingen3D_Training_PointNet_XYZ.npy'\n",
    "filename_lab = 'Vaihingen3D_Training_PointNet_Labels.npy'\n",
    "\n",
    "# Load the point clouds with the  x,y,z-values\n",
    "X = np.load(os.path.join(root_dir, filename_xyz))\n",
    "print(f'Successfully read \"{filename_xyz}\" feature matrix of shape {X.shape}')\n",
    "\n",
    "# Load labels\n",
    "y = np.load(os.path.join(root_dir, filename_lab))\n",
    "print(f'Successfully read \"{filename_lab}\" label vector of shape  {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.02],\n",
       "       [ 0.07,  0.15,  0.  ],\n",
       "       [ 0.04, -0.25, -0.02],\n",
       "       [-0.3 , -0.2 , -0.03],\n",
       "       [ 0.36, -0.1 ,  0.  ],\n",
       "       [-0.01, -0.38, -0.01],\n",
       "       [ 0.01,  0.38, -0.02],\n",
       "       [ 0.38,  0.27,  0.  ],\n",
       "       [ 0.1 ,  0.55, -0.01],\n",
       "       [ 0.35, -0.48,  0.02],\n",
       "       [-0.32, -0.56, -0.03],\n",
       "       [ 0.01, -0.66, -0.02],\n",
       "       [ 0.39,  0.64, -0.01],\n",
       "       [-0.02, -0.77,  0.02],\n",
       "       [ 0.34, -0.85,  0.03],\n",
       "       [ 0.4 ,  1.01, -0.01],\n",
       "       [ 1.15,  0.02,  0.  ],\n",
       "       [ 1.13, -0.35,  0.01],\n",
       "       [ 1.18,  0.39, -0.01],\n",
       "       [ 1.25, -0.14, -0.01],\n",
       "       [ 1.1 , -0.71,  0.02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see, e.g., the point cloud of the third point (at index 2)\n",
    "# (the elevation of the first point should be 0.02)\n",
    "# this is the input for each training and prediction pass\n",
    "\n",
    "X[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 1**:\n",
    "For this exercise, the data has already been prepared by finding the neighbours for each point in the point cloud and treating that as a sample. How could you process a point cloud in order create a dataset like that, specifically, which data structure would help you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a permutation of indices (shuffle indices)\n",
    "p = np.random.default_rng().permutation(X.shape[0])\n",
    "\n",
    "# Use the permutated indices to shuffle the array of features\n",
    "X = np.take(X, p, axis=0)\n",
    "\n",
    "# Use the (same) permutated indices to shuffle the array of labels\n",
    "# (It is very important to use the same indices for both features and labels.)\n",
    "y = np.take(y, p, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the PointNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please construct the PointNet model in the following cell. The comments will guide you through the construction. If you want to do it without any guidance, then just delete the comments and start from scratch.\n",
    "\n",
    "**Note:** You can implement the PointNet model either with 1D or 2D convolutional layers, but the explanations that follow are for using 2D convolutional layers. Your suggested task in the end will be to also implement PointNet with the other type of convolutional layers. So, you should do it in both ways to get more familiar with convolutional filters.\n",
    "\n",
    "In order to use a 2D convolutional layer, the input tensor must have 4 dimensions: batch size (B), number of points(N), 1 (in our case to make it 4 dimensional), and channels (C). Using the dimensions in this way, the filter size is (1, 1). If you decide to exchange the 3rd and 4th dimension, and you end up with BxNx3x1, then your filter size is (1,3). Remember that you do not provide the last dimension (the number of channels) in the convolutional filter. Either way, after the first layer, the network will be the same, and the features derived through the convolutional layers will be in the last dimension. Then, (1,1) convolutional filters are used. The dimensions of the input need to be provided to the Input layer.\n",
    "\n",
    "The implementation of the PointNet model follows the architecture presented on the lecture slides. The layers of the MLPs are each composed of a 2D convolutional layer, followed by batch normalization, and a ReLU activation layer. After 5 layers for the first MLP, there is a max pooling layer, and another (second) MLP with 3 layers. In between the 2nd and the 3rd layer (of the second MLP), a dropout layer with a dropout rate of 0.3 can help the training process. The last layer of the second MLP does not have batch normalization or an activation layer. Or rather, a softmax activation layer is used to generate the 9 class scores. \n",
    "\n",
    "Either before or after the softmax activation layer, you need to reduce the dimensions of the tensor from Bx1x1x9 to Bx1x9 by reshaping it to (1,9). Otherwise, the loss function will not work properly.\n",
    "\n",
    "I suggest you use the functional API to define the neural network model as is already given in the following cell. Then, you can add further information from the point cloud like the intensity as an additional input. Your (optional) task later on will be to inject intensity, number of returns, and return number that is also provided by the ISPRS data for the point cloud into the network after the feature extraction (after max pooling). In the functional model, you need to provide the sequence of layers by defining the output of the previous layer to be the input of the current layer.\n",
    "\n",
    "If you have difficulties with the functional API, then you can also use the sequential API. But we have not looked into if you can then inject further information as another input layer into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"PointNet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_xyz (InputLayer)      [(None, 21, 1, 3)]        0         \n",
      "                                                                 \n",
      " 1_Conv2D (Conv2D)           (None, 21, 1, 64)         256       \n",
      "                                                                 \n",
      " 1_BN (BatchNormalization)   (None, 21, 1, 64)         256       \n",
      "                                                                 \n",
      " 1_ReLU (Activation)         (None, 21, 1, 64)         0         \n",
      "                                                                 \n",
      " 2_Conv2D (Conv2D)           (None, 21, 1, 64)         4160      \n",
      "                                                                 \n",
      " 2_BN (BatchNormalization)   (None, 21, 1, 64)         256       \n",
      "                                                                 \n",
      " 2_ReLU (Activation)         (None, 21, 1, 64)         0         \n",
      "                                                                 \n",
      " 3_Conv2D (Conv2D)           (None, 21, 1, 64)         4160      \n",
      "                                                                 \n",
      " 3_BN (BatchNormalization)   (None, 21, 1, 64)         256       \n",
      "                                                                 \n",
      " 3_ReLU (Activation)         (None, 21, 1, 64)         0         \n",
      "                                                                 \n",
      " 4_Conv2D (Conv2D)           (None, 21, 1, 128)        8320      \n",
      "                                                                 \n",
      " 4_BN (BatchNormalization)   (None, 21, 1, 128)        512       \n",
      "                                                                 \n",
      " 4_ReLU (Activation)         (None, 21, 1, 128)        0         \n",
      "                                                                 \n",
      " 5_Conv2D (Conv2D)           (None, 21, 1, 1024)       132096    \n",
      "                                                                 \n",
      " 5_BN (BatchNormalization)   (None, 21, 1, 1024)       4096      \n",
      "                                                                 \n",
      " 5_ReLU (Activation)         (None, 21, 1, 1024)       0         \n",
      "                                                                 \n",
      " Max_Pool (MaxPooling2D)     (None, 1, 1, 1024)        0         \n",
      "                                                                 \n",
      " 6_FC (Conv2D)               (None, 1, 1, 512)         524800    \n",
      "                                                                 \n",
      " 6_BN (BatchNormalization)   (None, 1, 1, 512)         2048      \n",
      "                                                                 \n",
      " 6_ReLU (Activation)         (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " 7_FC (Conv2D)               (None, 1, 1, 256)         131328    \n",
      "                                                                 \n",
      " 7_BN (BatchNormalization)   (None, 1, 1, 256)         1024      \n",
      "                                                                 \n",
      " 7_ReLU (Activation)         (None, 1, 1, 256)         0         \n",
      "                                                                 \n",
      " DropOut (Dropout)           (None, 1, 1, 256)         0         \n",
      "                                                                 \n",
      " 8_FC (Conv2D)               (None, 1, 1, 9)           2313      \n",
      "                                                                 \n",
      " Reshape (Reshape)           (None, 1, 9)              0         \n",
      "                                                                 \n",
      " 8_Softmax (Activation)      (None, 1, 9)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 815,881\n",
      "Trainable params: 811,657\n",
      "Non-trainable params: 4,224\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 21:53:33.810144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14744 MB memory:  -> device: 4, name: Quadro RTX 5000, pci bus id: 0000:81:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import *\n",
    "\n",
    "#input layer that takes input of shape Batch size x Number of points x 1 x 3 channel (B, N, 1, 3)\n",
    "input_xyz = keras.layers.Input(shape=(21, 1, 3), dtype='float32', name='Input_xyz')\n",
    "\n",
    "# first layer of first MLP with 64 filters\n",
    "\n",
    "conv1 = keras.layers.Conv2D(filters=64, kernel_size=(1, 1), name ='1_Conv2D')(input_xyz)\n",
    "bn1   = keras.layers.BatchNormalization(name='1_BN')(conv1)\n",
    "relu1 = keras.layers.Activation('relu', name='1_ReLU')(bn1)\n",
    "\n",
    "# second layer of first MLP with 64 filters\n",
    "\n",
    "conv2 = keras.layers.Conv2D(filters=64, kernel_size=(1, 1), name ='2_Conv2D')(relu1)\n",
    "bn2   = keras.layers.BatchNormalization(name='2_BN')(conv2)\n",
    "relu2 = keras.layers.Activation('relu', name='2_ReLU')(bn2)\n",
    "\n",
    "# third layer of first MLP with 64 filters\n",
    "\n",
    "conv3 = keras.layers.Conv2D(filters=64, kernel_size=(1, 1), name ='3_Conv2D')(relu2)\n",
    "bn3   = keras.layers.BatchNormalization(name='3_BN')(conv3)\n",
    "relu3 = keras.layers.Activation('relu', name='3_ReLU')(bn3)\n",
    "\n",
    "# fourth layer of first MLP with 128 filters\n",
    "\n",
    "conv4 = keras.layers.Conv2D(filters=128, kernel_size=(1, 1), name ='4_Conv2D')(relu3)\n",
    "bn4   = keras.layers.BatchNormalization(name='4_BN')(conv4)\n",
    "relu4 = keras.layers.Activation('relu', name='4_ReLU')(bn4)\n",
    "\n",
    "# fifth layer of first MLP with 1024 filters\n",
    "\n",
    "conv5 = keras.layers.Conv2D(filters=1024, kernel_size=(1, 1), name ='5_Conv2D')(relu4)\n",
    "bn5  = keras.layers.BatchNormalization(name='5_BN')(conv5)\n",
    "relu5 = keras.layers.Activation('relu', name='5_ReLU')(bn5)\n",
    "\n",
    "\n",
    "# max pooling, the pool size is (21, 1)\n",
    "\n",
    "pool = keras.layers.MaxPool2D(pool_size=(21, 1), name='Max_Pool')(relu5)\n",
    "\n",
    "# first layer of the second MLP with 512 filters\n",
    "# (note: the second MLP could also be implemented with dense layers, but then the \n",
    "#        tensor needs to be reshaped to get rid of the extra dimension using (1, 1024).)\n",
    "\n",
    "fc6 = keras.layers.Conv2D(filters=512, kernel_size=(1, 1), name='6_FC')(pool)\n",
    "bn6 = keras.layers.BatchNormalization(name='6_BN')(fc6)\n",
    "relu6 = keras.layers.Activation('relu', name='6_ReLU')(bn6)\n",
    "\n",
    "\n",
    "# second layer of the second MLP with 256 filters\n",
    "fc7 = keras.layers.Conv2D(filters=256, kernel_size=(1, 1), name='7_FC')(relu6)\n",
    "bn7 = keras.layers.BatchNormalization(name='7_BN')(fc7)\n",
    "relu7 = keras.layers.Activation('relu', name='7_ReLU')(bn7)\n",
    "\n",
    "# insert a dropout layer with rate 0.3\n",
    "\n",
    "dropout = keras.layers.Dropout(rate=0.3, name='DropOut')(relu7)\n",
    "\n",
    "# third layer of the second MLP with 9 filters (for classification scores)\n",
    "# WITHOUT batch normalization and without activation function\n",
    "#...\n",
    "\n",
    "fc8 = keras.layers.Conv2D(filters=9, kernel_size=(1, 1), name='8_FC')(dropout)\n",
    "\n",
    "# the tensor needs to be reshaped to (1, 9) to get rid of extra dimension\n",
    "# (If you decided to implement the second MLP with dense layers, then you do not\n",
    "#  need to perform a reshape here as you already did earlier.)\n",
    "#...\n",
    "\n",
    "reshape = keras.layers.Reshape((1, 9), name='Reshape')(fc8)\n",
    "\n",
    "# softmax actication layer\n",
    "\n",
    "softmax = keras.layers.Activation('softmax', name='8_Softmax')(reshape)\n",
    "\n",
    "# Functional model with defined inputs and outputs\n",
    "model = keras.Model(inputs=[input_xyz], outputs=[softmax], name='PointNet')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "The dataset is currently in the shape (753876, 21, 3). But the first dimension of the training data is the number of training instances (753876) and not a dimension that is used for training. The network will randomly use one (or a whole batch) of these 753876 training instances and provide it to the network. Then, the data instance is of shape (21,3). But the network expects a tensor of shape (21,1,3). (The first dimension is the batch size, and we do not need to provide that ourselves.). Therefore, we need to expand the dimensions of the tensor in the second to last (-2) dimension. (We do not actually change the data itself, we just change the definition of the tensor. No data is added and no extra memory is required.)\n",
    "\n",
    "Use maybe 20 (to 40 epochs) for training the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before expand: (753876, 21, 3)\n",
      "Shape after expand:  (753876, 21, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape before expand: {X.shape}\\nShape after expand:  {np.expand_dims(X, axis=-2).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 21:53:35.022337: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inPointNet/DropOut/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-01-31 21:53:35.700687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
      "2024-01-31 21:53:35.767921: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x560740c012b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-31 21:53:35.768011: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Quadro RTX 5000, Compute Capability 7.5\n",
      "2024-01-31 21:53:35.840528: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9424/9424 [==============================] - 42s 4ms/step - loss: 0.7980 - accuracy: 0.6586 - val_loss: 0.7526 - val_accuracy: 0.6626\n",
      "Epoch 2/20\n",
      "9424/9424 [==============================] - 37s 4ms/step - loss: 0.7162 - accuracy: 0.6965 - val_loss: 0.6932 - val_accuracy: 0.7082\n",
      "Epoch 3/20\n",
      "9424/9424 [==============================] - 38s 4ms/step - loss: 0.6922 - accuracy: 0.7061 - val_loss: 0.6778 - val_accuracy: 0.7117\n",
      "Epoch 4/20\n",
      "9424/9424 [==============================] - 37s 4ms/step - loss: 0.6774 - accuracy: 0.7124 - val_loss: 0.6644 - val_accuracy: 0.7180\n",
      "Epoch 5/20\n",
      "9424/9424 [==============================] - 37s 4ms/step - loss: 0.6673 - accuracy: 0.7166 - val_loss: 0.6266 - val_accuracy: 0.7328\n",
      "Epoch 6/20\n",
      "9424/9424 [==============================] - 37s 4ms/step - loss: 0.6576 - accuracy: 0.7197 - val_loss: 0.6291 - val_accuracy: 0.7276\n",
      "Epoch 7/20\n",
      "9424/9424 [==============================] - 39s 4ms/step - loss: 0.6511 - accuracy: 0.7222 - val_loss: 0.6413 - val_accuracy: 0.7269\n",
      "Epoch 8/20\n",
      "9424/9424 [==============================] - 39s 4ms/step - loss: 0.6444 - accuracy: 0.7257 - val_loss: 0.6394 - val_accuracy: 0.7211\n",
      "Epoch 9/20\n",
      "9424/9424 [==============================] - 39s 4ms/step - loss: 0.6385 - accuracy: 0.7278 - val_loss: 0.6165 - val_accuracy: 0.7362\n",
      "Epoch 10/20\n",
      "9424/9424 [==============================] - 39s 4ms/step - loss: 0.6339 - accuracy: 0.7297 - val_loss: 0.6334 - val_accuracy: 0.7260\n",
      "Epoch 11/20\n",
      "9424/9424 [==============================] - 39s 4ms/step - loss: 0.6292 - accuracy: 0.7310 - val_loss: 0.6416 - val_accuracy: 0.7199\n",
      "Epoch 12/20\n",
      "9424/9424 [==============================] - 39s 4ms/step - loss: 0.6245 - accuracy: 0.7332 - val_loss: 0.6047 - val_accuracy: 0.7391\n",
      "Epoch 13/20\n",
      "9424/9424 [==============================] - 38s 4ms/step - loss: 0.6200 - accuracy: 0.7349 - val_loss: 0.6130 - val_accuracy: 0.7357\n",
      "Epoch 14/20\n",
      "9424/9424 [==============================] - 39s 4ms/step - loss: 0.6174 - accuracy: 0.7356 - val_loss: 0.6155 - val_accuracy: 0.7332\n",
      "Epoch 15/20\n",
      "9424/9424 [==============================] - 39s 4ms/step - loss: 0.6140 - accuracy: 0.7368 - val_loss: 0.5911 - val_accuracy: 0.7469\n",
      "Epoch 16/20\n",
      "9424/9424 [==============================] - 39s 4ms/step - loss: 0.6103 - accuracy: 0.7388 - val_loss: 0.5874 - val_accuracy: 0.7475\n",
      "Epoch 17/20\n",
      "9424/9424 [==============================] - 39s 4ms/step - loss: 0.6075 - accuracy: 0.7394 - val_loss: 0.5937 - val_accuracy: 0.7447\n",
      "Epoch 18/20\n",
      "9424/9424 [==============================] - 38s 4ms/step - loss: 0.6044 - accuracy: 0.7407 - val_loss: 0.5908 - val_accuracy: 0.7473\n",
      "Epoch 19/20\n",
      "9424/9424 [==============================] - 39s 4ms/step - loss: 0.6014 - accuracy: 0.7423 - val_loss: 0.6061 - val_accuracy: 0.7284\n",
      "Epoch 20/20\n",
      "9424/9424 [==============================] - 39s 4ms/step - loss: 0.5987 - accuracy: 0.7429 - val_loss: 0.6625 - val_accuracy: 0.7124\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64  # you can increase this for faster training\n",
    "\n",
    "# train directly on numpy array\n",
    "history = model.fit(np.expand_dims(X, axis=-2), y, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=20, \n",
    "                    validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Vaihingen evaluation data\n",
    "\n",
    "In this exercise, we use the evaluation part of the ISPRS dataset for real testing, so that we get some realistic numbers for quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read \"Vaihingen3D_Evaluation_PointNet_XYZ.npy\" feature matrix of shape (411722, 21, 3)\n",
      "Successfully read \"Vaihingen3D_Evaluation_PointNet_Labels.npy\" label vector of shape  (411722, 1)\n"
     ]
    }
   ],
   "source": [
    "filename_xyz = 'Vaihingen3D_Evaluation_PointNet_XYZ.npy'\n",
    "filename_lab = 'Vaihingen3D_Evaluation_PointNet_Labels.npy'\n",
    "\n",
    "# Load the point clouds with the  x,y,z-values\n",
    "X_test = np.load(os.path.join(root_dir, filename_xyz))\n",
    "print(f'Successfully read \"{filename_xyz}\" feature matrix of shape {X_test.shape}')\n",
    "\n",
    "# Load labels\n",
    "y_test = np.load(os.path.join(root_dir, filename_lab))\n",
    "print(f'Successfully read \"{filename_lab}\" label vector of shape  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test dataset should be around 68%. This low accuracy is to be expected and should be approximately the same as with the hand-craftet features. However, the network now extracts features by itself and we provided less information to it than in the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6434/6434 [==============================] - 10s 2ms/step - loss: 0.9893 - accuracy: 0.6384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9893084168434143, 0.6384380459785461]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.expand_dims(X_test, axis=-2), y_test, \n",
    "               batch_size=BATCH_SIZE, \n",
    "               verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, some typical evaluation reports are given.\n",
    "\n",
    "**confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12867/12867 [==============================] - 14s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Powerline</th>\n",
       "      <th>Low vegetation</th>\n",
       "      <th>Impervious surfaces</th>\n",
       "      <th>Car</th>\n",
       "      <th>Fence/Hedge</th>\n",
       "      <th>Roof</th>\n",
       "      <th>Facade</th>\n",
       "      <th>Shrub</th>\n",
       "      <th>Tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Powerline</th>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>55</td>\n",
       "      <td>35</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low vegetation</th>\n",
       "      <td>0</td>\n",
       "      <td>49802</td>\n",
       "      <td>38234</td>\n",
       "      <td>277</td>\n",
       "      <td>348</td>\n",
       "      <td>887</td>\n",
       "      <td>289</td>\n",
       "      <td>6288</td>\n",
       "      <td>2565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Impervious surfaces</th>\n",
       "      <td>0</td>\n",
       "      <td>16408</td>\n",
       "      <td>85131</td>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>124</td>\n",
       "      <td>32</td>\n",
       "      <td>221</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car</th>\n",
       "      <td>0</td>\n",
       "      <td>818</td>\n",
       "      <td>87</td>\n",
       "      <td>1170</td>\n",
       "      <td>111</td>\n",
       "      <td>244</td>\n",
       "      <td>9</td>\n",
       "      <td>1236</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fence/Hedge</th>\n",
       "      <td>0</td>\n",
       "      <td>1435</td>\n",
       "      <td>58</td>\n",
       "      <td>91</td>\n",
       "      <td>719</td>\n",
       "      <td>718</td>\n",
       "      <td>255</td>\n",
       "      <td>2977</td>\n",
       "      <td>1169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roof</th>\n",
       "      <td>3</td>\n",
       "      <td>1154</td>\n",
       "      <td>551</td>\n",
       "      <td>28</td>\n",
       "      <td>170</td>\n",
       "      <td>66184</td>\n",
       "      <td>2245</td>\n",
       "      <td>2038</td>\n",
       "      <td>36675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facade</th>\n",
       "      <td>5</td>\n",
       "      <td>899</td>\n",
       "      <td>74</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>325</td>\n",
       "      <td>5006</td>\n",
       "      <td>1215</td>\n",
       "      <td>3641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shrub</th>\n",
       "      <td>0</td>\n",
       "      <td>4295</td>\n",
       "      <td>477</td>\n",
       "      <td>332</td>\n",
       "      <td>261</td>\n",
       "      <td>1643</td>\n",
       "      <td>361</td>\n",
       "      <td>11758</td>\n",
       "      <td>5691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tree</th>\n",
       "      <td>10</td>\n",
       "      <td>1252</td>\n",
       "      <td>81</td>\n",
       "      <td>54</td>\n",
       "      <td>70</td>\n",
       "      <td>2855</td>\n",
       "      <td>1102</td>\n",
       "      <td>5800</td>\n",
       "      <td>43002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Powerline  Low vegetation  Impervious surfaces   Car  \\\n",
       "Powerline                   87               2                    0     0   \n",
       "Low vegetation               0           49802                38234   277   \n",
       "Impervious surfaces          0           16408                85131    17   \n",
       "Car                          0             818                   87  1170   \n",
       "Fence/Hedge                  0            1435                   58    91   \n",
       "Roof                         3            1154                  551    28   \n",
       "Facade                       5             899                   74    29   \n",
       "Shrub                        0            4295                  477   332   \n",
       "Tree                        10            1252                   81    54   \n",
       "\n",
       "                     Fence/Hedge   Roof  Facade  Shrub   Tree  \n",
       "Powerline                      1     28      55     35    392  \n",
       "Low vegetation               348    887     289   6288   2565  \n",
       "Impervious surfaces           42    124      32    221     11  \n",
       "Car                          111    244       9   1236     33  \n",
       "Fence/Hedge                  719    718     255   2977   1169  \n",
       "Roof                         170  66184    2245   2038  36675  \n",
       "Facade                        30    325    5006   1215   3641  \n",
       "Shrub                        261   1643     361  11758   5691  \n",
       "Tree                          70   2855    1102   5800  43002  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "class_names = ['Powerline', 'Low vegetation', 'Impervious surfaces', 'Car', 'Fence/Hedge', 'Roof', 'Facade', 'Shrub', 'Tree']\n",
    "\n",
    "y_test_pred = np.argmax(model.predict(np.expand_dims(X_test, axis=-2)), axis=-1)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "df = pd.DataFrame(data=cm, columns=class_names, index=class_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precision, recall, F1-score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.68\n",
      "Recall   : 0.64\n",
      "F1       : 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall   : {recall:.2f}')\n",
    "print(f'F1       : {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "          Powerline       0.83      0.14      0.25       600\n",
      "     Low vegetation       0.65      0.50      0.57     98690\n",
      "Impervious surfaces       0.68      0.83      0.75    101986\n",
      "                Car       0.59      0.32      0.41      3708\n",
      "        Fence/Hedge       0.41      0.10      0.16      7422\n",
      "               Roof       0.91      0.61      0.73    109048\n",
      "             Facade       0.54      0.45      0.49     11224\n",
      "              Shrub       0.37      0.47      0.42     24818\n",
      "               Tree       0.46      0.79      0.58     54226\n",
      "\n",
      "           accuracy                           0.64    411722\n",
      "          macro avg       0.60      0.47      0.48    411722\n",
      "       weighted avg       0.68      0.64      0.64    411722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save colorized point cloud of testing data\n",
    "\n",
    "For outputting the point cloud with the predicted class labels, we need the original 3D point cloud with original x,y,z-coordinates. The points are in the same order as the NumPy arrays that provide the x,y,z-coordinates of the small point clouds (with the 21-points) and the labels. (Remember that the points in the small point clouds are all centered by the first point, so that we do not have the original coordinates anymore.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read \"Vaihingen3D_Evaluation.pts\" of shape (411722, 3)\n"
     ]
    }
   ],
   "source": [
    "filename_pc = 'Vaihingen3D_Evaluation.pts'\n",
    "\n",
    "xyz_df = pd.read_csv(os.path.join(root_dir, filename_pc), \n",
    "                     sep=\" \", \n",
    "                     names=['x', 'y', 'z', 'intensity', 'return_number', 'number_of_returns', 'label'],\n",
    "                     usecols=['x', 'y', 'z'],\n",
    "                     header=None)\n",
    "\n",
    "print(f'Successfully read \"{filename_pc}\" of shape {xyz_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved \"Vaihingen3D_Evaluation_Results.csv\"\n"
     ]
    }
   ],
   "source": [
    "save_colorized_point_cloud(xyz_df.to_numpy(), y_test_pred, 'Vaihingen3D_Evaluation_Results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, we used PointNet as a feature encoder for points with their 20-point neighborhood for the classification of 3D point clouds. The feature encoding is the part until (including) max pooling. What follows is a typical classification network with fully connected layers that are implemented with convolutional filters and with softmax . (A convolutional filter that has the same size as the input has the same effect as a fully connected layer. Only the interpretation of the data is different with regard to tensor dimensions.)\n",
    "\n",
    "As seen by the results, the quality is approximately the same as with hand-crafted features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer to Question 1**:\n",
    "A KD-Tree would be a good data structure, which you might know from the GeoInformatics lecture. It enables efficient queries for finding the neighbours of a point. It is for instance implemented in the sklearn package:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Below are a few ideas for tasks that could still be done for PointNet in order to practice more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Predictions on large dataset\n",
    "\n",
    "Use the large dataset for prediction and outputting the colorized point cloud. The files are called \"Vaihingen3D_Large_PointNet_XYZ.npy\" for the small input point clouds. And the points with the original x,y,z-coordinates are stored in \"Vaihingen3D_Large_Points.csv.gz\". As there are no labels in this large dataset, it can only be used for prediction and not for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - 1D Convolutions\n",
    "\n",
    "Implement the PointNet model also with 1D convolutions. Be careful with the tensor dimensions (input data, input and filter sizes, reshape?, etc.). When working with 1D convolutions, your whole model needs to process data with 3 dimensions: batch size, number of points, channels. (You do not need to provide batch sizes, so the definition of layers is without the batch size dimension.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Intensity, return number, number of returns\n",
    "\n",
    "You can include the 3 other columns of the dataset, 'intensity', 'return_number', and 'number_of_returns', as further (second) input to the network and inject this data into the network after the max pooling layer. The following cells should give you some information on how to implement this.\n",
    "\n",
    "Explanation on the sensor features:\n",
    "- Intensity is the intensity with which the laser beam was reflected. Flat, impervious surfaces typically have a higher intensity as vegetation.\n",
    "- The laser beam is sometimes reflected several times, e.g. going through a tree, the laser beam is reflected at the branches, and several returns are registered and digitized. Number of returns is the total number of these reflections.\n",
    "- The laser beam is typically not going straight downwards, so there is a tilt in its direction. The different returns (of the same beam) at different elevation levels, lead to 3D points with different x,y-coordinates. Therefore, all points have individual coordinates and the return number denotes the \"index\" of the return it was derived from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the point cloud data with intensity, return number, number of returns. Do not forget to also shuffle it with the same permutation as the xyz and labels. But make absolutely sure that the shape of the tensor does not change. Check with the .shape() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19.  1.  1.]\n",
      " [24.  1.  1.]\n",
      " [31.  1.  1.]\n",
      " ...\n",
      " [36.  1.  1.]\n",
      " [36.  1.  1.]\n",
      " [39.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pc_df = pd.read_csv(os.path.join(root_dir, 'Vaihingen3D_Training.pts'), \n",
    "                     sep=\" \", \n",
    "                     names=['x', 'y', 'z', 'intensity', 'return_number', 'number_of_returns', 'label'],\n",
    "                     header=None)\n",
    "\n",
    "intensity = pc_df[['intensity', 'return_number', 'number_of_returns']].to_numpy(dtype=float)\n",
    "\n",
    "print(intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a second input layer (right after the first input layer) with input dimensions (3). (Depending on your implementation of PointNet, you will need to reshape the tensor within the nextwork before the concatenation with the extracted features.) In the summary, your input layer will only appear if it is actually used and at the position where it is used. So please be aware that you might not find it right away or at a position you might not have expected it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'Input_intensity')>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_intensity = keras.layers.Input(shape=(3), dtype='float32', name='Input_intensity')\n",
    "input_intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert a concatenation layer to concatenate the (1024) channels of the extracted features and with the (3) channels of the intensity input. But before that, you have to adjust the dimensions of the second input, so that it fits the global features.\n",
    "\n",
    "If your output dimensions from the max pooling layer are (None, 1, 1, 1024) and the output dimensions of your intensity are (None, 1, 3), then reshape your intensity to (None, 1, 1, 3).\n",
    "\n",
    "Then concatenate the output of the pooling layer with the reshape layer of the intensity. Do not forget to change the next layer to take the new concat output as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_intensity = keras.layers.Reshape((1, 1, 3), name='ReshapeIntensity')(input_intensity)\n",
    "\n",
    "concat = keras.layers.Concatenate(name='Concatenate')([pool,reshape_intensity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model must be defined to have two input layers. Just exchange the respective line in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[input_xyz, input_intensity], outputs=[softmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting the model, you have to provide a tuple (a pair) of the inputs, by using the parenthesis (for the tuple) and the two input Numpy arrays. (You could also expand the dimension of the intensity input before it is inputted it into the fit() function instead of reshaping it in the model. The effect, however, is the same.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 22:12:14.657691: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_1/DropOut/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11780/11780 [==============================] - 47s 4ms/step - loss: 0.5986 - accuracy: 0.7431\n",
      "Epoch 2/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5948 - accuracy: 0.7448\n",
      "Epoch 3/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5926 - accuracy: 0.7457\n",
      "Epoch 4/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5896 - accuracy: 0.7469\n",
      "Epoch 5/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5863 - accuracy: 0.7482\n",
      "Epoch 6/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5840 - accuracy: 0.7496\n",
      "Epoch 7/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5821 - accuracy: 0.7497\n",
      "Epoch 8/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5789 - accuracy: 0.7514\n",
      "Epoch 9/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5772 - accuracy: 0.7521\n",
      "Epoch 10/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5745 - accuracy: 0.7530\n",
      "Epoch 11/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5722 - accuracy: 0.7539\n",
      "Epoch 12/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5695 - accuracy: 0.7554\n",
      "Epoch 13/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5682 - accuracy: 0.7554\n",
      "Epoch 14/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5662 - accuracy: 0.7562\n",
      "Epoch 15/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5640 - accuracy: 0.7575\n",
      "Epoch 16/20\n",
      "11780/11780 [==============================] - 46s 4ms/step - loss: 0.5613 - accuracy: 0.7590\n",
      "Epoch 17/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5594 - accuracy: 0.7595\n",
      "Epoch 18/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5579 - accuracy: 0.7603\n",
      "Epoch 19/20\n",
      "11780/11780 [==============================] - 46s 4ms/step - loss: 0.5554 - accuracy: 0.7611\n",
      "Epoch 20/20\n",
      "11780/11780 [==============================] - 45s 4ms/step - loss: 0.5536 - accuracy: 0.7617\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit((np.expand_dims(X, axis=-2), intensity), y,batch_size=BATCH_SIZE, \n",
    "                    epochs=20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that you also need to provide a tuple as input for the predictions you make. The evaluation dataset for testing is called \"Vaihingen3D_Evaluation.pts\", from which you need to extract the intensity, etc. for the predictions.\n",
    "\n",
    "Your accuracy on the evaluation dataset should get up about 8% to 76%. As you can see, this kind of sensor information can be to quite an improvement. The added value will, however, dimish once we continue with multi-scale feature extraction.\n",
    "\n",
    "But also output the colorized point cloud and check what really changed. You can also take the labeled file \"Vaihingen3D_Training.pts\", extract the x,y,z-coordinates, and the labels as two numpy arrays, and save it with the helper function as a colorized point cloud as a referenc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
